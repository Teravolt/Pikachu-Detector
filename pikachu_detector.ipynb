{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Pikachu in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import Image\n",
    "\n",
    "from PIL import ImageDraw\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import GradientAccumulationPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import set_seed\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() \\\n",
    "        else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    run_name='pikachu-detector',\n",
    "    model_name='pikachu-detector-baseline-model',\n",
    "    image_size=256,\n",
    "    hidden_dims=256,\n",
    "    horizontal_flip_prob=0.5,\n",
    "    gaussian_blur_kernel_size=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=4e-4,\n",
    "    seed=1,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    lr_exp_schedule_gamma=0.85,\n",
    "    lr_warmup_steps=500,\n",
    "    train_limit=-1,\n",
    "    save_model=True,\n",
    "    mixed_precision=None,\n",
    "    grad_accumulation_steps=4\n",
    "    )\n",
    "CONFIG.device = DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    \"\"\"\n",
    "    Build HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = None\n",
    "    with open('archive/annotations.json', 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    dataframe = pd.DataFrame.from_dict(json_data).T.reset_index()\n",
    "    dataframe.image = dataframe.image.map(lambda x: f\"archive/images/{x}\")\n",
    "    dataframe = dataframe.drop(['id', 'index'], axis=1)\n",
    "    dataframe['x_1'] = dataframe['loc'].map(lambda x: x[0])\n",
    "    dataframe['y_1'] = dataframe['loc'].map(lambda x: x[1])\n",
    "    dataframe['x_2'] = dataframe['loc'].map(lambda x: x[2])\n",
    "    dataframe['y_2'] = dataframe['loc'].map(lambda x: x[3])\n",
    "\n",
    "    dataframe['orientation_x'] = dataframe.x_2 - dataframe.x_1\n",
    "    dataframe['orientation_y'] = dataframe.y_2 - dataframe.y_1\n",
    "\n",
    "    orientations = []\n",
    "    orientation_x = dataframe['orientation_x'].values\n",
    "    orientation_y = dataframe['orientation_y'].values\n",
    "\n",
    "    for x, y in zip(orientation_x, orientation_y):\n",
    "        orientation = 0 if x > 0 and y > 0 else \\\n",
    "            1 if x < 0 and y > 0 else \\\n",
    "                2 if x < 0 and y < 0 else \\\n",
    "                    3 if x > 0 and y < 0 else -1\n",
    "        orientations.append(orientation)\n",
    "    \n",
    "    dataframe['orientation'] = orientations\n",
    "    # dataframe['orientation'] = pd.DataFrame.where(dataframe.orientation_x > 0 and dataframe.orientation_y > 0, 0)\n",
    "    # Define orientation of the Pikachu\n",
    "\n",
    "    dataset = Dataset.from_pandas(dataframe).cast_column('image', Image())\n",
    "    return _, dataset\n",
    "\n",
    "def prepare_dataloader(config: Namespace):\n",
    "    \"\"\"\n",
    "    Prepare dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    train_preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config.image_size, config.image_size)),  # Resize\n",
    "            transforms.RandomHorizontalFlip(p=config.horizontal_flip_prob),\n",
    "            transforms.GaussianBlur(kernel_size=config.gaussian_blur_kernel_size),\n",
    "            transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "            transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "        ])\n",
    "\n",
    "    val_preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config.image_size, config.image_size)),  # Resize\n",
    "            transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "            transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "        ])\n",
    "\n",
    "    # For pre-processing original image for visualization in W&Bs\n",
    "    preprocess_original = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((512, 512)),  # Resize\n",
    "            transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        ])\n",
    "\n",
    "    _, dataset = build_dataset()\n",
    "\n",
    "    # Remove images that are 100x100 or below.\n",
    "    dataset = \\\n",
    "        dataset.filter(\n",
    "            lambda example: example['image'].size[0] > 100 and example['image'].size[1] > 100)\n",
    "\n",
    "    def train_transform(examples):\n",
    "        images = [train_preprocess(image.convert('RGB')) for image in examples['image']]\n",
    "        original_images = [\n",
    "            preprocess_original(image.convert('RGB')) \\\n",
    "                for image in examples['image']]\n",
    "\n",
    "        bounding_boxes = torch.stack([torch.tensor(ex) for ex in examples['loc']], dim=0)\n",
    "        return {'image': images,\n",
    "                'bounding_box': bounding_boxes,\n",
    "                'original-image': original_images\n",
    "                }\n",
    "\n",
    "    def val_transform(examples):\n",
    "        images = [val_preprocess(image.convert('RGB')) for image in examples['image']]\n",
    "        original_images = [\n",
    "            preprocess_original(image.convert('RGB')) \\\n",
    "                for image in examples['image']]\n",
    "        \n",
    "        bounding_boxes = torch.stack([torch.tensor(ex) for ex in examples['loc']], dim=0)\n",
    "        return {'image': images,\n",
    "                'bounding_box': bounding_boxes,\n",
    "                'original-image': original_images\n",
    "                }\n",
    "\n",
    "    # Split dataset into train + val. Balance train + val\n",
    "\n",
    "\n",
    "    # num_points = len(dataset)\n",
    "\n",
    "    # How do we balance when we have a regression problem?\n",
    "    # labels = dataset['labels']\n",
    "\n",
    "    # split_df = pd.DataFrame()\n",
    "    # split_df['labels'] = labels\n",
    "    # split_df['id'] = list(range(num_points))\n",
    "    # split_df['fold'] = -1\n",
    "\n",
    "    # cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.seed)\n",
    "    # for i, (_, test_ids) in enumerate(cv.split(np.zeros(num_points), labels)):\n",
    "    #     split_df.loc[test_ids, ['fold']] = i\n",
    "\n",
    "    # split_df['split'] = 'train'\n",
    "    # split_df.loc[split_df.fold == 0, ['split']] = 'val'\n",
    "\n",
    "    # print(split_df[split_df['split'].str.fullmatch('train')].labels.value_counts())\n",
    "    # print(split_df[split_df['split'].str.fullmatch('val')].labels.value_counts())\n",
    "\n",
    "    # train_indices = split_df[split_df['split'].str.fullmatch('train')]['id']\n",
    "    # val_indices = split_df[split_df['split'].str.fullmatch('val')]['id']\n",
    "\n",
    "    # def train_generator():\n",
    "    #     for idx in train_indices:\n",
    "    #         yield dataset['train'][idx]\n",
    "\n",
    "    # def val_generator():\n",
    "    #     for idx in val_indices:\n",
    "    #         yield dataset['train'][idx]\n",
    "\n",
    "    np_generator = np.random.default_rng(config.seed)\n",
    "    train_val_dataset = dataset.train_test_split(test_size=0.2, shuffle=True,\n",
    "                                                 generator=np_generator)\n",
    "    train_dataset = train_val_dataset['train']\n",
    "    val_dataset = train_val_dataset['test']\n",
    "\n",
    "    train_dataset.set_transform(train_transform)\n",
    "    val_dataset.set_transform(val_transform)\n",
    "\n",
    "    train_gen = torch.Generator().manual_seed(config.seed)\n",
    "    val_gen = torch.Generator().manual_seed(config.seed)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.per_device_train_batch_size,\n",
    "        shuffle=True, generator=train_gen)\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=config.per_device_eval_batch_size,\n",
    "        shuffle=False, generator=val_gen)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of models are used for detection?\n",
    "# Input: Image\n",
    "# Output: (x, y), length, width OR the diagnoals of the rectangular window (x1, y1), (x2, y2)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class PikachuDetector(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(\n",
    "            in_channels, dims, kernel_size=12)\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(\n",
    "            dims, 2*dims, kernel_size=5)\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3)\n",
    "        self.max_pool_3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_4 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3)\n",
    "        self.max_pool_4 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.projection = torch.nn.LazyLinear(4*dims)\n",
    "\n",
    "        # num_subsets = int(np.log2(dims)) + 2\n",
    "\n",
    "        # print(f\"Number of subsets: {num_subsets} - Dimensions: {4*dims}\")\n",
    "\n",
    "        # self.linear_layers = torch.nn.ModuleList()\n",
    "        # for i in range(3, num_subsets+1):\n",
    "        #     # print(f\"Number of dimensions: {2**i}\")\n",
    "        #     self.linear_layers.append(\n",
    "        #         torch.nn.Linear(2**i, num_labels))\n",
    "        # Top-left corner: (x1, y1), and bottom-right corner: (x2, y2)\n",
    "        self.regression_head = torch.nn.Linear(4*dims, 4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        x_ = self.conv_1(x)\n",
    "        x_ = self.max_pool_1(x_)\n",
    "        # print(f\"Output of conv & max pool 1: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_2(x_)\n",
    "        x_ = self.max_pool_2(x_)\n",
    "        # print(f\"Output of conv & max pool 2: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_3(x_)\n",
    "        x_ = self.max_pool_3(x_)\n",
    "        # print(f\"Output of conv & max pool 3: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_4(x_)\n",
    "        x_ = self.max_pool_4(x_)\n",
    "        # print(f\"Output of conv & max pool 4: {x_.shape}\")\n",
    "\n",
    "        x_ = self.flatten(x_)\n",
    "        # print(f\"Output of flatten: {x_.shape}\")\n",
    "\n",
    "        x_ = self.projection(x_)\n",
    "        # print(f\"Output of projection: {x_.shape}\")\n",
    "\n",
    "        output = self.regression_head(x_)\n",
    "\n",
    "        # output = self.sigmoid(x_)\n",
    "        # output = (output + 1)/2.0\n",
    "\n",
    "        # output = []\n",
    "        # for i, layer in enumerate(self.linear_layers):\n",
    "        #     # print(f\"Getting slice: {2**(3+i)} - {x_[:, 0:2**(3+i)].shape}\")\n",
    "        #     x__ = layer(x_[:, 0:2**(3+i)])\n",
    "        #     # print(f\"Subset output: {x__.shape}\")\n",
    "        #     output.append(x__)\n",
    "\n",
    "        # output = torch.stack(output, dim=1)\n",
    "        # print(f\"Final output: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "def create_model(in_dimensions: int, dims: int):\n",
    "    \"\"\"\n",
    "    Create model\n",
    "    \"\"\"\n",
    "\n",
    "    model = PikachuDetector(in_dimensions, dims)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "\n",
    "def compute_loss(preds: torch.Tensor, gts: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute MSE loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Sum over each subset & average over each batch\n",
    "    loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    # Cross entropy loss require (batch_size, x1 y1 x2 y2, ...)\n",
    "    loss = loss_fn(preds, gts)\n",
    "    return loss\n",
    "\n",
    "def add_bounding_box_to_image(img: PIL.Image, bounding_box: torch.Tensor,\n",
    "                              img_height: int, img_width: int):\n",
    "    \"\"\"\n",
    "    Add bounding box to an image\n",
    "    \"\"\"\n",
    "\n",
    "    x_1 = bounding_box[0]*img_height\n",
    "    y_1 = bounding_box[1]*img_width\n",
    "\n",
    "    x_2 = bounding_box[2]*img_height\n",
    "    y_2 = bounding_box[3]*img_width\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    start_x = min(x_1, x_2)\n",
    "    start_y = min(y_1, y_2)\n",
    "    end_x = max(x_1, x_2)\n",
    "    end_y = max(y_1, y_2)\n",
    "\n",
    "    draw.rectangle(((start_x, start_y), (end_x, end_y)))\n",
    "\n",
    "    return draw._image\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop(epoch: int, model, dataloader,\n",
    "              wandb_run, accelerator: Accelerator):\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "\n",
    "    tensor_to_pil = transforms.ToPILImage()\n",
    "\n",
    "    columns = ['epoch', 'img_gt_bounding_box', 'img_pred_bounding_box',\n",
    "               'model_image',  'pred_x1', 'pred_y1', 'pred_x2', 'pred_y2',\n",
    "               'gt_x1', 'gt_y1', 'gt_x2', 'gt_y2']\n",
    "    dataframe = []\n",
    "\n",
    "    avg_loss = 0\n",
    "    for _, batch in enumerate(dataloader):\n",
    "\n",
    "        pred_bounding_box = model(batch['image'])\n",
    "        gt_bounding_box = batch['bounding_box']\n",
    "\n",
    "        loss = compute_loss(pred_bounding_box, gt_bounding_box)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        images = []\n",
    "        img_gt_bounding_box = []\n",
    "        img_pred_bounding_box = []\n",
    "\n",
    "        for j in range(batch['image'].shape[0]):\n",
    "            images.append(tensor_to_pil(batch['image'][j,:]))\n",
    "\n",
    "            gt_original_img = tensor_to_pil(batch['original-image'][j,:])\n",
    "            pred_original_img = tensor_to_pil(batch['original-image'][j,:])\n",
    "\n",
    "            gt_bound_box_img = add_bounding_box_to_image(gt_original_img, gt_bounding_box[j,:],\n",
    "                                                         gt_original_img.height, gt_original_img.width)\n",
    "            pred_bound_box_img = add_bounding_box_to_image(pred_original_img, pred_bounding_box[j,:],\n",
    "                                                           pred_original_img.height, pred_original_img.width)\n",
    "\n",
    "            img_gt_bounding_box.append(gt_bound_box_img)\n",
    "            img_pred_bounding_box.append(pred_bound_box_img)\n",
    "\n",
    "        batch_dataframe = pd.DataFrame(columns=columns)\n",
    "        batch_dataframe['epoch'] = [epoch for _ in range(len(images))]\n",
    "        batch_dataframe['model_image'] = \\\n",
    "            [wandb.Image(image) for image in images]\n",
    "        batch_dataframe['img_gt_bounding_box'] = \\\n",
    "            [wandb.Image(image) for image in img_gt_bounding_box]\n",
    "        batch_dataframe['img_pred_bounding_box'] = \\\n",
    "            [wandb.Image(image) for image in img_pred_bounding_box]\n",
    "\n",
    "        batch_dataframe['pred_x1'] = pred_bounding_box[:, 0].tolist()\n",
    "        batch_dataframe['pred_y1'] = pred_bounding_box[:, 1].tolist()\n",
    "        batch_dataframe['pred_x2'] = pred_bounding_box[:, 2].tolist()\n",
    "        batch_dataframe['pred_y2'] = pred_bounding_box[:, 3].tolist()\n",
    "\n",
    "        batch_dataframe['gt_x1'] = gt_bounding_box[:, 0].tolist()\n",
    "        batch_dataframe['gt_y1'] = gt_bounding_box[:, 1].tolist()\n",
    "        batch_dataframe['gt_x2'] = gt_bounding_box[:, 2].tolist()\n",
    "        batch_dataframe['gt_y2'] = gt_bounding_box[:, 3].tolist()\n",
    "        dataframe.append(batch_dataframe)\n",
    "\n",
    "    dataframe = pd.concat(dataframe, axis=0, ignore_index=True)\n",
    "    # dataframe.to_csv('testing.csv')\n",
    "    # Get average accuracy and loss\n",
    "    # acc = (dataframe['gt'] == dataframe['pred']).mean()\n",
    "    avg_loss = avg_loss/len(dataloader)\n",
    "\n",
    "    accelerator.print(\n",
    "        f\"Val MSE loss: {avg_loss}\")\n",
    "\n",
    "    table = wandb.Table(data=dataframe)\n",
    "    # wandb_run.log({'accuracy': acc}, commit=False)\n",
    "    wandb_run.log({'val-mse-loss': loss}, commit=False)\n",
    "    wandb_run.log({'eval-table': table})\n",
    "\n",
    "def training_loop(config: Namespace):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "\n",
    "    wandb_run = wandb.init(project='Pikachu-Detector', entity=None,\n",
    "                           job_type='training',\n",
    "                           name=config.run_name,\n",
    "                           config=config)\n",
    "\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    grad_accumulation_plugin = GradientAccumulationPlugin(\n",
    "        num_steps=config.grad_accumulation_steps,\n",
    "        adjust_scheduler=True,\n",
    "        sync_with_dataloader=True)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_plugin=grad_accumulation_plugin,\n",
    "        cpu=(config.device == 'cpu'))\n",
    "\n",
    "    train_dataloader, val_dataloader = prepare_dataloader(config)    \n",
    "    model = create_model(3, config.hidden_dims)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "#     scheduler = CosineAnnealingLR(\n",
    "#         optimizer,\n",
    "#         T_max=config.num_train_epochs)\n",
    "    scheduler = ExponentialLR(\n",
    "        optimizer,\n",
    "        config.lr_exp_schedule_gamma)\n",
    "\n",
    "#     scheduler = CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=config.lr_warmup_steps)\n",
    "        # last_epoch=config.num_train_epochs*len(train_dataloader))\n",
    "\n",
    "    model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, val_dataloader, scheduler)\n",
    "\n",
    "    num_steps = 0\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        model.train()\n",
    "\n",
    "        accelerator.print(f\"Epoch {epoch}\")\n",
    "\n",
    "        epoch_loss = 0\n",
    "        num_iters = 0\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                pred_bounding_box = model(batch['image'])\n",
    "                gt_bounding_box = batch['bounding_box']\n",
    "\n",
    "                loss = compute_loss(pred_bounding_box, gt_bounding_box)\n",
    "\n",
    "                # accelerator.print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                wandb_run.log({'loss': loss.item()}, commit=False, step=num_steps)\n",
    "                wandb_run.log({'lr': scheduler.get_lr()[0]}, commit=False, step=num_steps)\n",
    "\n",
    "                num_steps += 1\n",
    "                num_iters += 1\n",
    "\n",
    "                # Update the model parameters with the optimizer\n",
    "                optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validate model\n",
    "        accelerator.print(\"Evaluating model\")\n",
    "        eval_loop(epoch, model, val_dataloader, wandb_run, accelerator)\n",
    "\n",
    "        wandb_run.log({'epoch-loss': epoch_loss/num_iters})\n",
    "\n",
    "    if config.save_model:\n",
    "        # Save model to W&Bs\n",
    "        model_art = wandb.Artifact(config.model_name, type='model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "        model_art.add_file('model.pt')\n",
    "        wandb_run.log_artifact(model_art)\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# MODEL = create_model(3, CONFIG.hidden_dims)\n",
    "# train_dataloader, val_dataloader = prepare_dataloader(CONFIG)\n",
    "# eval_loop(0, MODEL, val_dataloader, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, (CONFIG, ), num_processes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pikachu-detection-r3Yh-UUI-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
